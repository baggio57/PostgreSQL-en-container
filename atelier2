D’autres ressources sont quant à elles créées lors de la création d’une instance PostgreSQL.
Nous l’avons vu par exemple avec la commande "kubectl get pod" qui nous retournait deux lignes correspondant à deux Pods.
Pour n’en citer que quelques-unes : Pod, Secret, ConfigMap … et des Services.

Qu’est-ce qu’un Service ?
Tout d’abord, un Pod est volatile par définition et lorsqu’il est recréé, il obtient une nouvelle adresse IP. 
Comment un client (i.e application) pourrait-il accéder de manière pérenne à ce Pod (par exemple, une instance PostgreSQL) alors que l’adresse IP change ? 
On pourrait imaginer passer par une résolution DNS, avec la modification de l’adresse IP, mais encore faut-il que l’actualisation DNS se fasse assez rapidement…

Un Service se charge d’acheminer les connexions clientes vers l’application demandée. Les clients se connectent donc à ce composant et ce dernier choisit où 
transmettre le flux réseau en se basant sur des labels positionnés sur le ou les Pods concernés.

Ainsi, un Service est donc plus “stable” dans le temps qu’un Pod. Nos clients (i.e applications) peuvent donc utiliser son nom DNS (ou l’adresse IP) pour accéder aux applications. 
Je vous renvoie sur la documentation Kubernetes pour découvrir ce qu’est un Service et les différents types qui existent.

CloudNativePG crée trois services pour chaque cluster PostgreSQL déployé. Par exemple, pour le cluster que nous avons déployé la dernière fois, il existe :

    un service qui permet d’accéder au primaire, en lecture/écriture : postgresql-rw ;
    un service qui permet d’accéder uniquement au(x) secondaire(s), en lecture seule : postgresql-ro ;
    un service qui permet d’accéder à toutes les instances : postgresql-r.

La pratique vaut souvent plus qu’un long discours. Regardons cela avec notre cluster PostgreSQL précédent et l’application pgAdmin4.

-----------
fichier yaml pour deployer pgAdmin4 :

apiVersion: apps/v1
kind: Deployment
metadata:
  name: pgadmin
spec:
  replicas: 1
  selector:
    matchLabels:
      app: pgadmin
  template:
    metadata:
      labels:
        app: pgadmin
    spec:
      containers:
        - name: pgadmin
          image: dpage/pgadmin4
          ports:
            - containerPort: 80
          env:
            - name: PGADMIN_DEFAULT_EMAIL
              value: admin@example.com
            - name: PGADMIN_DEFAULT_PASSWORD
              value: admin

-----------------
Toujours la meme commande pour deployer : kubectl apply -f .\pgadmin4.yaml

Pour verifier son deployement : kubectl get pods

NAME                       READY   STATUS    RESTARTS   AGE
pgadmin-7989b7989f-sfdkz   1/1     Running   0          42s
postgresql-1               1/1     Running   0          4m59s

Lorsque pgAdmin4 est déployé, je l’expose avec la commande kubectl port-forward.
Cela me permet d’atteindre l’interface web depuis mon navigateur en allant sur http://localhost:7777.

kubectl port-forward --address 0.0.0.0 $(kubectl get pod -o name | grep pgadmin) 7777:80 => attention cette commande ne focntionne pas sous powershell !!!

Sous windows :
kubectl port-forward --address 0.0.0.0 pgadmin-7989b7989f-sfdkz 7777:80
Forwarding from 0.0.0.0:7777 -> 80

avec : pgadmin-7989b7989f-sfdkz le nom de mon pod "pgadmin4"

quand j'execute cette commande, j'ai l'erreur suivante :
Warning: would violate PodSecurity "restricted:v1.31": allowPrivilegeEscalation != false (container "pgadmin" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pgadmin" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pgadmin" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pgadmin" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
deployment.apps/pgadmin created

suite aux travaux effectues par olivier, j'ai modifie mon fichier yaml :
Pour que cela focntionne, il faut en plus creer un fichier ingress et un fichier service :

fichier deploimenent de pgadmin4 :
------
apiVersion: apps/v1
kind: Deployment
metadata:
  name: pgadmin
spec:
  replicas: 1
  selector:
    matchLabels:
      app: pgadmin
  template:
    metadata:
      labels:
        app: pgadmin
    spec:
      containers:
        - name: pgadmin
          image: dpage/pgadmin4
          ports:
            - containerPort: 80
          imagePullPolicy: Always
          securityContext:
            allowPrivilegeEscalation: true
          env:
            - name: PGADMIN_DEFAULT_EMAIL
              value: admin@insee.fr
            - name: PGADMIN_DEFAULT_PASSWORD
              value: adminpg
--------------
fichier ingress :
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: pgadmin-ingress
  namespace:
  labels:
    name: pgadmin-ingress
  annotations:                                                   # <- Cette ligne
    cert-manager.io/cluster-issuer: "developpement-insee-fr"     # <- Cette ligne
spec:
  tls:
    - hosts:
        - pgadmin.developpement.insee.fr
      secretName: pgadmin-ingress-cert                                # <- Cette ligne 
  rules:
    - host: pgadmin.developpement.insee.fr
      http:
        paths:
          - pathType: Prefix
            path: "/"
            backend:
              service:
                name: pgadmin
                port:
                  number: 80
------
fichier service :
--- 
apiVersion: v1
kind: Service
metadata:
  name: pgadmin
spec:
  selector:
    app: pgadmin
  ports:
  - port: 80
    targetPort: 80

Je n'ai plus d'erreur lorsque j'applique ces 3 nouveaux fichies.
J'accede bien à la console web de pgadmin4 => url = pgadmin.developpement.insee.fr
Le user plus mot de passe se trouve dans le fichier de deploiement de pgadmin4.

--------------------



